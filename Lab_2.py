# -*- coding: utf-8 -*-
"""Reuters_classified

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ViVPS6_QRtY0zdGTCcBPoIww9LcCq3pv
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from operator import itemgetter    
import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
get_ipython().magic(u'matplotlib inline')
plt.style.use('ggplot')

import warnings
warnings.filterwarnings('ignore')
from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve, average_precision_score, auc
#from sklearn.utils.fixes import signature
from sklearn.decomposition import PCA
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score


import tensorflow as tf
from keras import models, regularizers, layers, optimizers, losses, metrics
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, Activation, LSTM, GRU, SpatialDropout1D, Flatten
from keras.utils import np_utils, to_categorical
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

from keras.datasets import reuters

cb_EarlyStopping = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)
cb_ModelCheckpoint = ModelCheckpoint(filepath='model.best.hdf5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)
cb_ReduceLROnPlateau = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=3, verbose=1, mode='max', min_lr=0.00000001)

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

print("train_data ", train_data.shape)
print("train_labels ", train_labels.shape)
print("test_data ", test_data.shape)
print("test_labels ", test_labels.shape)

# Заменим числа в тренировочном датасете, чтобы видеть слова
# Заметинм, что индекс начинаются с 3, потому что 0,1,2 являются специальными для отступа, начала последовательности и неизвестного символа. 

word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
train_data[0]])

print(train_data[0])
print(decoded_newswire)
print(train_labels[0])

#Векторизация функция
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

# Векторизация и нормализация тренировочного и тестового датасетов

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

print("x_train ", x_train.shape)
print("x_test ", x_test.shape)

# Векторизация меток
one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)

print("one_hot_train_labels ", one_hot_train_labels.shape)
print("one_hot_test_labels ", one_hot_test_labels.shape)

# Задание валидационной выборки

x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]

print("x_val ", x_val.shape)
print("y_val ", y_val.shape)

print("partial_x_train ", partial_x_train.shape)
print("partial_y_train ", partial_y_train.shape)

# Модель сети
# Dropout слои помогают решить проблему преобучения

model = models.Sequential()
model.add(layers.Dense(512, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(128, kernel_regularizer=regularizers.l1(0.001), activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(46, activation='softmax'))
model.summary()

# Обучение модели

NumEpochs = 50
BatchSize = 64

model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(partial_x_train, partial_y_train, 
                    epochs=NumEpochs, 
                    batch_size=BatchSize, 
                    callbacks = [cb_EarlyStopping, cb_ModelCheckpoint, cb_ReduceLROnPlateau],
                    validation_split=0.1, 
                    #validation_data=(x_val, y_val),
                    verbose=1)

results = model.evaluate(x_val, y_val)
print("_"*100)
print("Test Loss and Accuracy")
print("results ", results)

history_dict = history.history
history_dict.keys()

plt.clf()
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Прогноз

predictions = model.predict(x_test)
# Каждый прогноз это вектор из 46 коэффициентов
print(predictions[123].shape)

# Сумма уоэффициентов должна быть равна единице
print(np.sum(predictions[123]))

# Наксимальный коэффициент соответствует предсказанному классу
print(np.argmax(predictions[123]))

# Получим 3 наиболее предсказываемых класса
predictions[21].argsort()[-3:][::-1]

test_labels[21]

SampleNum = 2125

print(test_labels[SampleNum])
print(predictions[SampleNum].argsort()[-3:][::-1])

test_labels[SampleNum] in predictions[SampleNum].argsort()[-3:][::-1]

Top3Preds = np.zeros((2246,3), dtype=int)
print(Top3Preds.shape)

for SampleNum in range(predictions.shape[0]):
    Top3Preds[SampleNum] = predictions[SampleNum].argsort()[-3:][::-1]
    
Top3Preds

FinalPreds = np.zeros((2246,1), dtype=int)
print(FinalPreds.shape)

for SampleNum in range(Top3Preds.shape[0]):
    if test_labels[SampleNum] in Top3Preds[SampleNum]:
        FinalPreds[SampleNum] = 1
        
FinalPreds

FinalPreds = pd.DataFrame(FinalPreds)
NumTop3 = FinalPreds[0][FinalPreds[0] == 1].count()
percentTop3 = round(100 *NumTop3 / FinalPreds.shape[0], 1)

print('Percent of one from top 3 being correct ... ', percentTop3, '%')

